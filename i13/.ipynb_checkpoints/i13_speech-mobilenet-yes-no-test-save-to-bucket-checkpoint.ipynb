{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "# File Processing\n",
    "import glob\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "# Math\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import math\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# Data Plotting and Visualizations\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "pd.options.display.max_rows = 40\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_WAVFILES_PATH = './train/audio/'\n",
    "TRAIN_SPECT_PATH = './train/spect/'\n",
    "TRAIN_SPECT_TMP_PATH = './train/spect_tmp_2/'\n",
    "#PREDICTION_LABELS = [\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", \"unknown\"]\n",
    "PREDICTION_LABELS = [\"yes\", \"no\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data - From Image Spectrograms -> Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0 - folder name: yes - number of files: 2377\n",
      "index: 1 - folder name: no - number of files: 2375\n"
     ]
    }
   ],
   "source": [
    "all_images = []\n",
    "subfolder_lens = []\n",
    "for x in range(len(PREDICTION_LABELS)):\n",
    "    subfolder_files = [x for x in os.listdir(TRAIN_SPECT_PATH + PREDICTION_LABELS[x]) if '.jpg' in x]\n",
    "    number_of_files = len(subfolder_files)\n",
    "    print(\"index: %d - folder name: %s - number of files: %d\" % (x, PREDICTION_LABELS[x], number_of_files))\n",
    "    number_of_files = 2350 #len(number_of_files)\n",
    "    subfolder_lens.append(number_of_files)\n",
    "    for y in range(number_of_files):\n",
    "        image_path = TRAIN_SPECT_PATH + PREDICTION_LABELS[x] + '/' + subfolder_files[y]\n",
    "        image = np.asarray(Image.open(image_path))\n",
    "        image = np.append(image, x)\n",
    "        all_images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_images = len(all_images)\n",
    "len_train_data = int(total_images *0.8)\n",
    "len_validation_data = int(total_images *0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_images = np.asarray(np.random.permutation(all_images))\n",
    "all_examples = all_images[:, :-1].reshape(-1, 96,96,3)\n",
    "all_labels = all_images[:, -1:]\n",
    "training_examples = all_examples[:len_train_data].astype(np.float32)\n",
    "validation_examples = all_examples[len_train_data:].astype(np.float32)\n",
    "training_labels = all_labels[:len_train_data]\n",
    "validation_labels = all_labels[len_train_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of original np array for all images:\n",
      "(4700, 27649)\n",
      "\n",
      "train/validation split\n",
      "train len : 3760\n",
      "validation len: 940\n",
      "\n",
      "training examples shape:\n",
      "(3760, 96, 96, 3)\n",
      "training labels shape:\n",
      "(3760, 1)\n",
      "validation examples shape:\n",
      "(940, 96, 96, 3)\n",
      "validation labels shape:\n",
      "(940, 1)\n"
     ]
    }
   ],
   "source": [
    "print('shape of original np array for all images:')\n",
    "print(all_images.shape)\n",
    "\n",
    "print(\"\\ntrain/validation split\")\n",
    "print('train len :', len_train_data)\n",
    "print('validation len:', len_validation_data)\n",
    "\n",
    "print('\\ntraining examples shape:')\n",
    "print(training_examples.shape)\n",
    "print('training labels shape:')\n",
    "print(training_labels.shape)\n",
    "print('validation examples shape:')\n",
    "print(validation_examples.shape)\n",
    "print('validation labels shape:')\n",
    "print(validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model and Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mobilenet_v2_35_model_fn(features, labels, mode, params):\n",
    "    module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_96/feature_vector/1\")\n",
    "    input_layer = features[\"x\"]\n",
    "    outputs = module(input_layer)\n",
    "    print(\"outputs:\")\n",
    "    print(outputs.shape)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=outputs, units=2)\n",
    "    print(\"flattened layer\")\n",
    "    print(logits.shape)\n",
    "    \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    print(\"predictions generated\")\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    print(\"Calculate Loss (for both TRAIN and EVAL modes)\")\n",
    "    print(labels.shape)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    print(\"Loss calculated.\")\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])\n",
    "        optimizer = tf.contrib.estimator.clip_gradients_by_norm(optimizer, 5.0)\n",
    "        print(\"minimizing loss\")\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_input_fn(data, targets, batch_size, num_epochs=None, shuffle=True):\n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": data},\n",
    "            y=targets,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_predict_input_fn(data, targets, batch_size):\n",
    "    input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": data},\n",
    "        y=targets,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mobilenet_model(learning_rate, steps, batch_size,\n",
    "                    training_examples, training_targets, \n",
    "                    validation_examples, validation_targets):\n",
    "    periods = 10\n",
    "    steps_per_period = steps // periods \n",
    "    \n",
    "    train_input_fn = create_train_input_fn(training_examples, training_targets, batch_size)\n",
    "    train_predictions_fn = create_predict_input_fn(training_examples, training_targets, batch_size)\n",
    "    validation_predictions_fn = create_predict_input_fn(validation_examples, validation_targets, batch_size)\n",
    "    \n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=mobilenet_v2_35_model_fn,\n",
    "        model_dir=\"./mobilenet_model_001_fn\",\n",
    "        params={\n",
    "            'learning_rate': learning_rate, \n",
    "        })\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    print(\"LogLoss error (on validation data):\")\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    train_accuracy_log = []\n",
    "    validation_accuracy_log = []\n",
    "    for period in range (0, periods):\n",
    "        #print(\"period %d\" % period)\n",
    "        classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=steps_per_period,\n",
    "            #hooks=[logging_hook]\n",
    "        )\n",
    "        \n",
    "        print(\"making training predictions\")\n",
    "        training_predictions = classifier.predict(input_fn=train_predictions_fn)\n",
    "        training_predictions_list = list(training_predictions)\n",
    "        #print(\"pulling out probabilities\")\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_predictions_list])\n",
    "        training_pred_class_id = np.array([item['classes'] for item in training_predictions_list])\n",
    "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,2)\n",
    "\n",
    "        print(\"making validation predictions\")\n",
    "        validation_predictions = classifier.predict(input_fn=validation_predictions_fn)\n",
    "        validation_predictions_list = list(validation_predictions)\n",
    "        #print(\"pulling out probabilities\")\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions_list])    \n",
    "        validation_pred_class_id = np.array([item['classes'] for item in validation_predictions_list])\n",
    "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,2)    \n",
    "\n",
    "        training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n",
    "        validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n",
    "        \n",
    "        train_accuracy = metrics.accuracy_score(training_targets, training_pred_class_id)\n",
    "        validation_accuracy = metrics.accuracy_score(validation_targets, validation_pred_class_id)\n",
    "        print(\"  period %02d : %0.2f, %0.2f\" % (period, validation_log_loss, validation_accuracy))\n",
    "        \n",
    "        train_accuracy_log.append(train_accuracy)\n",
    "        validation_accuracy_log.append(validation_accuracy)\n",
    "        training_errors.append(training_log_loss)\n",
    "        validation_errors.append(validation_log_loss)\n",
    "    print(\"Model training finished.\")\n",
    "\n",
    "    _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "\n",
    "    final_predictions = classifier.predict(input_fn=validation_predictions_fn)\n",
    "    final_predictions = np.array([item['classes'] for item in final_predictions])\n",
    "\n",
    "    accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n",
    "    print(\"Final accuracy (on validation data): %0.2f\" % accuracy)\n",
    "\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(training_errors, label=\"training\")\n",
    "    plt.plot(validation_errors, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"Accuracy vs. Periods\")\n",
    "    plt.plot(train_accuracy_log, label=\"accuracy\")\n",
    "    plt.plot(validation_accuracy_log, label=\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call to Execute Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "LogLoss error (on validation data):\n",
      "outputs:\n",
      "(100, 1280)\n",
      "flattened layer\n",
      "(100, 2)\n",
      "predictions generated\n",
      "Calculate Loss (for both TRAIN and EVAL modes)\n",
      "(100, 1)\n",
      "Loss calculated.\n",
      "minimizing loss\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mobilenet_trained_model = train_mobilenet_model(\n",
    "    learning_rate = 0.003,\n",
    "    steps=500,\n",
    "    batch_size=100,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_labels,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
